import os
import logging
from typing import List, Optional
from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader, TextLoader, CSVLoader, UnstructuredExcelLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from config.settings import settings, MODELS_DIR

# Parent-Child Index ä¾èµ–ï¼ˆå¼ºä¾èµ–å¤ç”¨ langchain_classicï¼‰
# ParentDocumentRetriever: å°å—æ£€ç´¢ï¼Œè¿”å›å¤§å—ä¸Šä¸‹æ–‡
from langchain_classic.retrievers import ParentDocumentRetriever
from langchain_classic.storage import LocalFileStore, create_kv_docstore

# EnsembleRetriever ä¾èµ–ï¼ˆå¼ºä¾èµ–å¤ç”¨ langchain_classicï¼‰
# EnsembleRetriever: Dense + BM25 åŒè·¯å¬å› + RRF èåˆï¼ˆå†…ç½®å®ç°ï¼‰
from langchain_classic.retrievers import EnsembleRetriever
from langchain_community.retrievers.bm25 import BM25Retriever

# ChildToParentBM25Retriever: å°† BM25 å­å—ç»“æœå‡çº§ä¸ºçˆ¶å—
from src.services.rag.child_to_parent_retriever import ChildToParentBM25Retriever

# Configure logging
logger = logging.getLogger(__name__)


def _get_reranker_device() -> str:
    """
    è‡ªåŠ¨æ£€æµ‹æœ€ä¼˜è®¡ç®—è®¾å¤‡ï¼ˆCPU/GPU/MPSï¼‰

    ä¾èµ–ï¼štorchï¼ˆç”± sentence-transformers è‡ªåŠ¨å®‰è£…ï¼‰
    è¿”å›ï¼š'cuda' | 'mps' | 'cpu'
    """
    import torch

    device_config = settings.RERANKER_DEVICE.lower()

    if device_config != "auto":
        return device_config

    # è‡ªåŠ¨æ£€æµ‹
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        logger.info(f"ğŸš€ Reranker: æ£€æµ‹åˆ° GPU - {device_name}")
        return "cuda"
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        logger.info("ğŸ Reranker: æ£€æµ‹åˆ° Apple MPS")
        return "mps"
    else:
        logger.info("ğŸ’» Reranker: ä½¿ç”¨ CPU")
        return "cpu"

class RAGPipeline:
    """
    Standalone RAG Pipeline for AI Teacher Nexus.
    White-box reuse of concepts from Agentic-RAG-Ollama, but implemented with local infrastructure.
    """
    def __init__(self, vector_db_path: str = "./data/chroma_db", chunking_strategy: str = "auto"):
        """
        :param vector_db_path: Path for ChromaDB persistence
        :param chunking_strategy: 'auto', 'header', or 'character'.
        """
        self.vector_db_path = vector_db_path
        
        # Initialize Embeddings (Aliyun/OpenAI Compatible)
        # Initialize Embeddings based on configuration
        if settings.EMBEDDING_PROVIDER == "local":
            from langchain_huggingface import HuggingFaceEmbeddings

            # HF_HOME is already set in config/settings.py, no need to set cache_folder
            # Try offline first, fallback to online download if model not found
            try:
                self.embeddings = HuggingFaceEmbeddings(
                    model_name=settings.EMBEDDING_MODEL,
                    model_kwargs={"local_files_only": True}
                )
                logger.info(f"Using Local Embeddings: {settings.EMBEDDING_MODEL} (offline mode)")
            except Exception as e:
                logger.warning(f"Model not found locally, downloading: {settings.EMBEDDING_MODEL}")
                self.embeddings = HuggingFaceEmbeddings(
                    model_name=settings.EMBEDDING_MODEL
                )
                logger.info(f"Using Local Embeddings: {settings.EMBEDDING_MODEL} (downloaded)")
        else:
            # Default to OpenAI/Aliyun
            self.embeddings = OpenAIEmbeddings(
                api_key=settings.OPENAI_API_KEY,
                base_url=settings.OPENAI_API_BASE,
                model=settings.EMBEDDING_MODEL,
                check_embedding_ctx_length=False
            )
            logger.info(f"Using OpenAI Embeddings: {settings.EMBEDDING_MODEL}")
        
        self.vectorstore = Chroma(
            collection_name="financial_docs", # Keep consistent with what we used
            persist_directory=self.vector_db_path,
            embedding_function=self.embeddings
        )
        self.chunking_strategy = chunking_strategy
        logger.info(f"Initialized RAGPipeline (Standalone, chunking_strategy={chunking_strategy})")

        # Reranker æ‡’åŠ è½½ï¼ˆé¦–æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–ï¼‰
        self._reranker = None
        self._reranker_initialized = False

        # ========== Parent-Child Index åˆå§‹åŒ– ==========
        # ä¾èµ–ï¼šlangchain_classic.retrievers.ParentDocumentRetrieverï¼ˆå®Œæ•´å¤ç”¨ï¼‰
        # å­˜å‚¨ï¼šlangchain.storage.LocalFileStoreï¼ˆæŒä¹…åŒ–çˆ¶å—åŸæ–‡ï¼‰
        self._parent_retriever = None
        self._parent_retriever_initialized = False

        # ========== BM25 Retriever åˆå§‹åŒ–ï¼ˆç”¨äº EnsembleRetrieverï¼‰==========
        # ä¾èµ–ï¼šlangchain_community.retrievers.BM25Retrieverï¼ˆå®Œæ•´å¤ç”¨ï¼‰
        # ç­–ç•¥ï¼šå¯åŠ¨æ—¶æ„å»ºï¼Œingest æ—¶åŒæ­¥å…¨é‡é‡å»ºï¼ˆå†…å­˜å¸¸é©»ï¼ŒæŸ¥è¯¢é›¶å»¶è¿Ÿï¼‰
        self._bm25_retriever = None

        # å¯åŠ¨æ—¶æ„å»º BM25ï¼ˆå¦‚æœ vectorstore æœ‰æ•°æ®ï¼‰
        self._build_bm25()

    @property
    def reranker(self):
        """
        æ‡’åŠ è½½ Rerankerï¼ˆCrossEncoderï¼‰

        ä¾èµ–ï¼šsentence-transformers.CrossEncoderï¼ˆå®Œæ•´å¤ç”¨ï¼Œä¸é‡å†™ï¼‰
        æ¨¡å‹ï¼šBAAI/bge-reranker-v2-m3ï¼ˆé€šè¿‡ settings é…ç½®ï¼‰
        è®¾å¤‡ï¼šè‡ªåŠ¨æ£€æµ‹ CPU/GPU/MPS
        """
        if self._reranker_initialized:
            return self._reranker

        self._reranker_initialized = True

        if not settings.RERANKER_ENABLED:
            logger.info("Reranker å·²ç¦ç”¨ (RERANKER_ENABLED=false)")
            return None

        try:
            # å¼ºä¾èµ–ï¼šsentence-transformers.CrossEncoder
            from sentence_transformers import CrossEncoder

            device = _get_reranker_device()

            # ä¼˜å…ˆå°è¯•ç¦»çº¿åŠ è½½ï¼ˆæ¨¡å‹å·²ä¸‹è½½åˆ° HF_HOME/MODELS_DIRï¼‰
            try:
                self._reranker = CrossEncoder(
                    settings.RERANKER_MODEL,
                    max_length=settings.RERANKER_MAX_LENGTH,
                    device=device,
                    trust_remote_code=True,
                    local_files_only=True
                )
                logger.info(f"âœ… Reranker åŠ è½½å®Œæˆ: {settings.RERANKER_MODEL} (ç¦»çº¿, {device})")
            except Exception:
                # ç¦»çº¿å¤±è´¥ï¼Œåœ¨çº¿ä¸‹è½½
                logger.info(f"Reranker æ¨¡å‹æœªæ‰¾åˆ°ï¼Œæ­£åœ¨ä¸‹è½½: {settings.RERANKER_MODEL}")
                self._reranker = CrossEncoder(
                    settings.RERANKER_MODEL,
                    max_length=settings.RERANKER_MAX_LENGTH,
                    device=device,
                    trust_remote_code=True
                )
                logger.info(f"âœ… Reranker åŠ è½½å®Œæˆ: {settings.RERANKER_MODEL} (å·²ä¸‹è½½, {device})")

        except ImportError:
            logger.warning("sentence-transformers æœªå®‰è£…ï¼ŒReranker ä¸å¯ç”¨")
            self._reranker = None
        except Exception as e:
            logger.error(f"Reranker åˆå§‹åŒ–å¤±è´¥: {e}")
            self._reranker = None

        return self._reranker

    @property
    def parent_retriever(self):
        """
        æ‡’åŠ è½½ ParentDocumentRetrieverï¼ˆçˆ¶å­ç´¢å¼•æ£€ç´¢å™¨ï¼‰

        ä¾èµ–ï¼šlangchain_classic.retrievers.ParentDocumentRetrieverï¼ˆå®Œæ•´å¤ç”¨ï¼Œä¸é‡å†™ï¼‰
        å­˜å‚¨ï¼šlangchain.storage.LocalFileStoreï¼ˆæŒä¹…åŒ–çˆ¶å—åŸæ–‡ï¼‰

        åŸç†ï¼š
        - å­å—ï¼ˆChildï¼‰ï¼šå°å—ï¼Œç”¨äºç²¾ç¡®å‘é‡æ£€ç´¢
        - çˆ¶å—ï¼ˆParentï¼‰ï¼šå¤§å—ï¼Œè¿”å›ç»™ LLM æä¾›å®Œæ•´ä¸Šä¸‹æ–‡

        è¯­ä¹‰åˆ†å—æ¨¡å¼ï¼ˆSEMANTIC_CHUNKING_ENABLED=trueï¼‰ï¼š
        - çˆ¶å—ä½¿ç”¨ Chonkie SemanticChunker è¿›è¡Œè¯­ä¹‰åˆ†å—
        - ä¾èµ–ï¼šchonkie.SemanticChunkerï¼ˆå®Œæ•´å¤ç”¨ï¼‰
        """
        if self._parent_retriever_initialized:
            return self._parent_retriever

        self._parent_retriever_initialized = True

        if not settings.PARENT_CHILD_ENABLED:
            logger.info("Parent-Child Index å·²ç¦ç”¨ (PARENT_CHILD_ENABLED=false)")
            return None

        try:
            # ç¡®ä¿ docstore ç›®å½•å­˜åœ¨
            docstore_path = settings.DOCSTORE_PATH
            docstore_path.mkdir(parents=True, exist_ok=True)

            # ä¾èµ–ï¼šlangchain_classic.storage.LocalFileStore + create_kv_docstore
            # LocalFileStore å­˜å‚¨ bytesï¼Œcreate_kv_docstore åŒ…è£…ä¸ºæ”¯æŒ Document çš„ docstore
            file_store = LocalFileStore(str(docstore_path))
            docstore = create_kv_docstore(file_store)

            # ========== çˆ¶å—åˆ†å‰²å™¨é€‰æ‹© ==========
            if settings.SEMANTIC_CHUNKING_ENABLED:
                # è¯­ä¹‰åˆ†å—æ¨¡å¼ï¼šä½¿ç”¨ Chonkie SemanticChunker
                # ä¾èµ–ï¼šchonkie.SemanticChunkerï¼ˆé€šè¿‡é€‚é…å™¨å®Œæ•´å¤ç”¨ï¼‰
                try:
                    from src.services.rag.semantic_splitter import ChonkieSemanticSplitter

                    # ç¡®å®š embedding æ¨¡å‹
                    embedding_model = settings.SEMANTIC_EMBEDDING_MODEL
                    if embedding_model.lower() == "auto":
                        embedding_model = settings.EMBEDDING_MODEL

                    parent_splitter = ChonkieSemanticSplitter(
                        embedding_model=embedding_model,
                        similarity_percentile=settings.SEMANTIC_SIMILARITY_PERCENTILE,
                        chunk_size=settings.SEMANTIC_CHUNK_SIZE,
                    )
                    logger.info(
                        f"âœ… è¯­ä¹‰åˆ†å—æ¨¡å¼: Chonkie SemanticChunker "
                        f"(model={embedding_model}, percentile={settings.SEMANTIC_SIMILARITY_PERCENTILE})"
                    )
                except ImportError as e:
                    logger.warning(f"Chonkie ä¸å¯ç”¨ï¼Œé™çº§ä¸ºå›ºå®šåˆ†å—: {e}")
                    parent_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=settings.PARENT_CHUNK_SIZE,
                        chunk_overlap=settings.PARENT_CHUNK_OVERLAP
                    )
                except Exception as e:
                    logger.warning(f"è¯­ä¹‰åˆ†å—åˆå§‹åŒ–å¤±è´¥ï¼Œé™çº§ä¸ºå›ºå®šåˆ†å—: {e}")
                    parent_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=settings.PARENT_CHUNK_SIZE,
                        chunk_overlap=settings.PARENT_CHUNK_OVERLAP
                    )
            else:
                # ä¼ ç»Ÿå›ºå®šåˆ†å—æ¨¡å¼
                parent_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=settings.PARENT_CHUNK_SIZE,
                    chunk_overlap=settings.PARENT_CHUNK_OVERLAP
                )
                logger.info(f"å›ºå®šåˆ†å—æ¨¡å¼: Parent({settings.PARENT_CHUNK_SIZE})")

            # å­å—åˆ†å‰²å™¨ï¼ˆå°å—ï¼Œç”¨äºå‘é‡æ£€ç´¢ï¼‰- å§‹ç»ˆä½¿ç”¨å›ºå®šåˆ†å—
            child_splitter = RecursiveCharacterTextSplitter(
                chunk_size=settings.CHILD_CHUNK_SIZE,
                chunk_overlap=settings.CHILD_CHUNK_OVERLAP
            )

            # ä¾èµ–ï¼šlangchain_classic.retrievers.ParentDocumentRetrieverï¼ˆå®Œæ•´å¤ç”¨ï¼‰
            self._parent_retriever = ParentDocumentRetriever(
                vectorstore=self.vectorstore,
                docstore=docstore,
                child_splitter=child_splitter,
                parent_splitter=parent_splitter,
            )

            logger.info(
                f"âœ… Parent-Child Index åˆå§‹åŒ–å®Œæˆ: "
                f"Parent({'Semantic' if settings.SEMANTIC_CHUNKING_ENABLED else settings.PARENT_CHUNK_SIZE}), "
                f"Child({settings.CHILD_CHUNK_SIZE})"
            )

        except Exception as e:
            logger.error(f"Parent-Child Index åˆå§‹åŒ–å¤±è´¥: {e}")
            self._parent_retriever = None

        return self._parent_retriever

    @property
    def bm25_retriever(self):
        """
        è·å– BM25Retrieverï¼ˆç¨€ç–æ£€ç´¢å™¨ï¼‰

        ä¾èµ–ï¼šlangchain_community.retrievers.BM25Retrieverï¼ˆå®Œæ•´å¤ç”¨ï¼Œä¸é‡å†™ï¼‰

        ç­–ç•¥ï¼š
        - å¯åŠ¨æ—¶ä» vectorstore å…¨é‡åŠ è½½å¹¶æ„å»ºï¼ˆ__init__ ä¸­è°ƒç”¨ _build_bm25ï¼‰
        - ingest æ—¶åŒæ­¥å…¨é‡é‡å»ºï¼ˆè°ƒç”¨ _build_bm25ï¼‰
        - å¸¸é©»å†…å­˜ï¼Œæ£€ç´¢æ—¶é›¶å»¶è¿Ÿ
        """
        return self._bm25_retriever

    def _build_bm25(self):
        """
        ä» vectorstore å…¨é‡é‡å»º BM25 ç´¢å¼•

        è°ƒç”¨æ—¶æœºï¼š
        - å¯åŠ¨æ—¶ï¼ˆ__init__ï¼‰
        - æ¯æ¬¡ ingest åï¼ˆåŒæ­¥é‡å»ºï¼‰

        æ€§èƒ½ï¼š
        - 1ä¸‡ç¯‡æ–‡æ¡£çº¦ å‡ ç™¾æ¯«ç§’ ~ 1ç§’
        - 10ä¸‡ç¯‡æ–‡æ¡£çº¦ å‡ ç§’
        - å®å¯ ingest æ…¢ 1ç§’ï¼Œä¹Ÿä¸è®© retrieve æ…¢ 1ç§’
        """
        try:
            data = self.vectorstore.get()

            if not data['documents']:
                logger.info("Vectorstore ä¸ºç©ºï¼ŒBM25Retriever å¾…é¦–æ¬¡ ingest åæ„å»º")
                self._bm25_retriever = None
                return

            # ä» vectorstore é‡å»º Document å¯¹è±¡åˆ—è¡¨
            docs = []
            for i, content in enumerate(data['documents']):
                metadata = data['metadatas'][i] if data['metadatas'] else {}
                docs.append(Document(page_content=content, metadata=metadata))

            # å…¨é‡é‡å»º BM25Retriever
            self._bm25_retriever = BM25Retriever.from_documents(docs)
            logger.info(f"âœ… BM25Retriever æ„å»ºå®Œæˆ: {len(docs)} æ–‡æ¡£")

        except Exception as e:
            logger.error(f"BM25Retriever æ„å»ºå¤±è´¥: {e}")
            self._bm25_retriever = None

    def _get_text_splitter(self, docs, file_path: str):
        """
        Use MarkdownHeaderTextSplitter if markdown, else fallback to RecursiveCharacterTextSplitter.
        """
        ext = os.path.splitext(file_path)[-1].lower()
        if self.chunking_strategy == "header" or (self.chunking_strategy == "auto" and ext in ['.md', '.markdown']):
            try:
                return MarkdownHeaderTextSplitter(headers_to_split_on=["#", "##", "###"], strip_headers=False)
            except Exception as e:
                logger.warning(f"Header splitter failed: {e}, falling back to character splitter.")
        # Fallback
        return RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

    def load_document(self, file_path: str) -> List[Document]:
        ext = os.path.splitext(file_path)[-1].lower()
        if ext == '.pdf':
            loader = PyPDFLoader(file_path)
        elif ext == '.docx':
            loader = UnstructuredWordDocumentLoader(file_path)
        elif ext == '.txt':
            loader = TextLoader(file_path, encoding="utf-8")
        elif ext == '.csv':
            loader = CSVLoader(file_path)
        elif ext == '.xlsx':
            loader = UnstructuredExcelLoader(file_path)
        elif ext in ['.md', '.markdown']:
             # Use TextLoader for markdown to keep raw text for header splitter
             loader = TextLoader(file_path, encoding="utf-8")
        else:
            raise ValueError(f"Unsupported file extension: {ext}")
        docs = loader.load()
        return docs

    def ingest(self, file_path: str, metadata: dict = None):
        """
        Ingests a file using adaptive chunking.

        å¦‚æœå¯ç”¨çˆ¶å­ç´¢å¼•ï¼ˆPARENT_CHILD_ENABLED=trueï¼‰ï¼š
        - ä½¿ç”¨ ParentDocumentRetriever.add_documentsï¼ˆè‡ªåŠ¨åˆ†çˆ¶å—/å­å—ï¼‰
        - å­å—å­˜å…¥å‘é‡åº“ï¼Œçˆ¶å—å­˜å…¥ DocStore

        å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿå•å±‚åˆ†å—ã€‚
        """
        docs = self.load_document(file_path)

        # é™„åŠ æ–‡ä»¶çº§ metadata
        for doc in docs:
            doc.metadata = doc.metadata or {}
            if metadata:
                doc.metadata.update(metadata)
            doc.metadata['source_file'] = file_path

        # ========== çˆ¶å­ç´¢å¼•æ¨¡å¼ ==========
        if self.parent_retriever is not None:
            # ä¾èµ–ï¼šParentDocumentRetriever.add_documentsï¼ˆå®Œæ•´å¤ç”¨ï¼‰
            # è‡ªåŠ¨å®Œæˆï¼šçˆ¶å—åˆ†å‰² â†’ å­å—åˆ†å‰² â†’ å­å—å‘é‡åŒ– â†’ çˆ¶å—å­˜å‚¨
            self.parent_retriever.add_documents(docs, ids=None)
            self._build_bm25()  # åŒæ­¥å…¨é‡é‡å»º BM25
            logger.info(f"âœ… [Parent-Child] Ingested from {file_path}")
            return

        # ========== ä¼ ç»Ÿå•å±‚åˆ†å—æ¨¡å¼ ==========
        splitter = self._get_text_splitter(docs, file_path)

        # MarkdownHeaderTextSplitter expects string, not documents
        if isinstance(splitter, MarkdownHeaderTextSplitter):
            text = "\n\n".join([d.page_content for d in docs])
            splits = splitter.split_text(text)
        else:
            splits = splitter.split_documents(docs)

        # Attach file-level metadata
        for doc in splits:
            doc.metadata = doc.metadata or {}
            if metadata:
                doc.metadata.update(metadata)
            doc.metadata['source_file'] = file_path

        self.vectorstore.add_documents(splits)
        self._build_bm25()  # åŒæ­¥å…¨é‡é‡å»º BM25
        logger.info(f"Ingested {len(splits)} chunks from {file_path}")

    def ingest_text(self, text: str, metadata: dict = None):
        """
        Ingest raw text.
        æ”¯æŒçˆ¶å­ç´¢å¼•æ¨¡å¼ (PARENT_CHILD_ENABLED=true)
        """
        doc = Document(page_content=text, metadata=metadata or {})
        docs = [doc]

        # ========== çˆ¶å­ç´¢å¼•æ¨¡å¼ ==========
        if self.parent_retriever is not None:
            # ä¾èµ–ï¼šParentDocumentRetriever.add_documentsï¼ˆå®Œæ•´å¤ç”¨ï¼‰
            self.parent_retriever.add_documents(docs, ids=None)
            self._build_bm25()  # åŒæ­¥å…¨é‡é‡å»º BM25
            logger.info(f"âœ… [Parent-Child] Ingested text ({len(text)} chars)")
            return

        # ========== ä¼ ç»Ÿæ¨¡å¼ ==========
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        splits = splitter.split_documents(docs)
        self.vectorstore.add_documents(splits)
        self._build_bm25()  # åŒæ­¥å…¨é‡é‡å»º BM25
        logger.info(f"Ingested {len(splits)} chunks from raw text")

    def retrieve(self, query: str, k: int = 4, keywords: Optional[list] = None, metadata_filter: Optional[dict] = None) -> List[Document]:
        """
        æ··åˆæ£€ç´¢ï¼šEnsembleRetriever (Dense + BM25 RRF èåˆ) + CrossEncoder é‡æ’åº

        å¦‚æœå¯ç”¨çˆ¶å­ç´¢å¼•ï¼ˆPARENT_CHILD_ENABLED=trueï¼‰ï¼š
        - ä½¿ç”¨ ParentDocumentRetriever æ£€ç´¢ï¼ˆå°å—åŒ¹é…ï¼Œè¿”å›å¤§å—ä¸Šä¸‹æ–‡ï¼‰

        æµç¨‹ï¼š
        1. EnsembleRetriever èåˆå¬å›ï¼ˆDense + BM25ï¼Œå†…ç½® RRF ç®—æ³•ï¼‰
        2. CrossEncoder ç²¾æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰

        ä¾èµ–ï¼š
        - langchain.retrievers.EnsembleRetrieverï¼ˆRRF èåˆï¼Œå®Œæ•´å¤ç”¨ï¼‰
        - langchain_community.retrievers.BM25Retrieverï¼ˆç¨€ç–æ£€ç´¢ï¼Œå®Œæ•´å¤ç”¨ï¼‰
        - langchain_classic.retrievers.ParentDocumentRetrieverï¼ˆçˆ¶å­ç´¢å¼•ï¼Œå®Œæ•´å¤ç”¨ï¼‰
        - sentence-transformers.CrossEncoderï¼ˆé‡æ’åºï¼Œå®Œæ•´å¤ç”¨ï¼‰

        æ³¨æ„ï¼škeywords å‚æ•°å·²å¼ƒç”¨ï¼ŒBM25Retriever å†…éƒ¨è‡ªåŠ¨å¤„ç†åˆ†è¯
        """
        # keywords å‚æ•°å·²å¼ƒç”¨è­¦å‘Š
        if keywords is not None:
            logger.warning("keywords å‚æ•°å·²å¼ƒç”¨ï¼ŒBM25Retriever å†…éƒ¨è‡ªåŠ¨å¤„ç†åˆ†è¯")

        use_reranker = self.reranker is not None
        use_parent_child = self.parent_retriever is not None

        # å¬å›æ›´å¤šå€™é€‰ç”¨äºé‡æ’åº
        fetch_k = k * 10 if use_reranker else k * 5

        # ========== 1. EnsembleRetriever èåˆå¬å› ==========
        try:
            candidates = self._ensemble_retrieve(query, fetch_k, use_parent_child, metadata_filter)
        except Exception as e:
            logger.warning(f"EnsembleRetriever å¤±è´¥ï¼Œé™çº§ä¸ºçº¯å‘é‡æ£€ç´¢: {e}")
            candidates = self._fallback_dense_retrieve(query, fetch_k, use_parent_child, metadata_filter)

        if not candidates:
            logger.info(f"No documents found for query: {query}")
            return []

        # ========== 2. CrossEncoder ç²¾æ’åº ==========
        if use_reranker and candidates:
            try:
                pairs = [[query, doc.page_content] for doc in candidates]
                scores = self.reranker.predict(pairs)

                ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
                results = [doc for doc, _ in ranked[:k]]

                logger.info(f"CrossEncoder é‡æ’åº: {len(candidates)} -> {len(results)} docs")
                return results

            except Exception as e:
                logger.warning(f"CrossEncoder é‡æ’åºå¤±è´¥: {e}")

        # é™çº§ï¼šç›´æ¥è¿”å› EnsembleRetriever ç»“æœ
        return candidates[:k]

    def _ensemble_retrieve(
        self,
        query: str,
        fetch_k: int,
        use_parent_child: bool,
        metadata_filter: Optional[dict] = None
    ) -> List[Document]:
        """
        ä½¿ç”¨ EnsembleRetriever è¿›è¡Œ Dense + BM25 èåˆæ£€ç´¢

        ä¾èµ–ï¼š
        - langchain.retrievers.EnsembleRetrieverï¼ˆå†…ç½® RRF èåˆï¼Œå®Œæ•´å¤ç”¨ï¼‰
        - ChildToParentBM25Retrieverï¼ˆParent-Child æ¨¡å¼ä¸‹å‡çº§å­å—ä¸ºçˆ¶å—ï¼‰

        Parent-Child æ¨¡å¼ç‰¹æ®Šå¤„ç†ï¼š
        - Dense è·¯å¾„ï¼šParentDocumentRetriever ç›´æ¥è¿”å›çˆ¶å—
        - BM25 è·¯å¾„ï¼šChildToParentBM25Retriever åŒ…è£…å™¨å°†å­å—å‡çº§ä¸ºçˆ¶å—
        - ç¡®ä¿ä¸¤æ¡è·¯å¾„è¿”å›ç›¸åŒç²’åº¦çš„æ–‡æ¡£ï¼ŒRRF èåˆç»“æœä¸€è‡´
        """
        # æ„å»º Dense Retriever
        if use_parent_child:
            # Parent-Child æ¨¡å¼ï¼šä½¿ç”¨ ParentDocumentRetriever
            dense_retriever = self.parent_retriever
        else:
            # ä¼ ç»Ÿæ¨¡å¼ï¼šä½¿ç”¨ vectorstore retriever
            dense_retriever = self.vectorstore.as_retriever(
                search_kwargs={"k": fetch_k, "filter": metadata_filter} if metadata_filter else {"k": fetch_k}
            )

        # è·å– BM25 Retriever
        raw_bm25_retriever = self.bm25_retriever

        if raw_bm25_retriever is None:
            # BM25 ä¸å¯ç”¨ï¼Œé™çº§ä¸ºçº¯å‘é‡æ£€ç´¢
            logger.info("BM25Retriever ä¸å¯ç”¨ï¼Œä½¿ç”¨çº¯å‘é‡æ£€ç´¢")
            results = dense_retriever.invoke(query)
            return results[:fetch_k]

        # ========== Parent-Child æ¨¡å¼ï¼šåŒ…è£… BM25 ä¸º ChildToParentBM25Retriever ==========
        if use_parent_child and self.parent_retriever is not None:
            # è·å– docstoreï¼ˆä» ParentDocumentRetriever å†…éƒ¨è·å–ï¼‰
            docstore = self.parent_retriever.docstore

            # åŒ…è£… BM25Retrieverï¼Œä½¿å…¶è¿”å›çˆ¶å—è€Œéå­å—
            bm25_retriever = ChildToParentBM25Retriever(
                bm25_retriever=raw_bm25_retriever,
                docstore=docstore,
                k=fetch_k
            )
            logger.info("Parent-Child æ¨¡å¼ï¼šä½¿ç”¨ ChildToParentBM25Retriever åŒ…è£…å™¨")
        else:
            # ä¼ ç»Ÿæ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨ BM25Retriever
            bm25_retriever = raw_bm25_retriever
            bm25_retriever.k = fetch_k

        # ä¾èµ–ï¼šlangchain.retrievers.EnsembleRetrieverï¼ˆå®Œæ•´å¤ç”¨ï¼‰
        # weights: [dense_weight, bm25_weight]ï¼Œé»˜è®¤å„ 0.5
        ensemble_retriever = EnsembleRetriever(
            retrievers=[dense_retriever, bm25_retriever],
            weights=[0.5, 0.5]  # RRF èåˆæƒé‡
        )

        results = ensemble_retriever.invoke(query)
        logger.info(f"EnsembleRetriever èåˆæ£€ç´¢: {len(results)} docs (Dense + BM25 RRF)")

        return results[:fetch_k]

    def _fallback_dense_retrieve(
        self,
        query: str,
        fetch_k: int,
        use_parent_child: bool,
        metadata_filter: Optional[dict] = None
    ) -> List[Document]:
        """
        é™çº§æ£€ç´¢ï¼šä»…ä½¿ç”¨ Denseï¼ˆå‘é‡ï¼‰æ£€ç´¢
        """
        if use_parent_child:
            results = self.parent_retriever.invoke(query)
        else:
            retriever = self.vectorstore.as_retriever(
                search_kwargs={"k": fetch_k, "filter": metadata_filter} if metadata_filter else {"k": fetch_k}
            )
            results = retriever.invoke(query)

        logger.info(f"é™çº§ Dense æ£€ç´¢: {len(results)} docs")
        return results[:fetch_k]

    def update_metadata(self, source_file: str, metadata_updates: dict) -> bool:
        """
        Update metadata for all chunks of a document without re-embedding.
        Much faster than delete + re-ingest.

        Args:
            source_file: The source file path to identify chunks
            metadata_updates: Dict of metadata fields to update

        Returns:
            True if successful, False otherwise
        """
        import time
        start_time = time.time()

        try:
            # Get existing chunks
            t1 = time.time()
            data = self.vectorstore.get(where={"source_file": source_file})
            t2 = time.time()
            logger.info(f"[TIMING] vectorstore.get took {t2-t1:.3f}s")

            chunk_ids = data['ids']

            if not chunk_ids:
                logger.warning(f"No documents found for source_file: {source_file}")
                return False

            # Get existing metadata and merge updates
            existing_metadatas = data['metadatas']
            updated_metadatas = []
            for meta in existing_metadatas:
                updated_meta = {**meta, **metadata_updates}
                updated_metadatas.append(updated_meta)

            # Update metadata directly in ChromaDB (no re-embedding)
            t3 = time.time()
            self.vectorstore._collection.update(
                ids=chunk_ids,
                metadatas=updated_metadatas
            )
            t4 = time.time()
            logger.info(f"[TIMING] collection.update took {t4-t3:.3f}s")

            total_time = time.time() - start_time
            logger.info(f"Updated metadata for {len(chunk_ids)} chunks: {source_file} (total: {total_time:.3f}s)")
            return True
        except Exception as e:
            logger.error(f"Error updating metadata for {source_file}: {e}")
            return False

    def delete_document(self, source_file: str):
        """
        Delete all chunks associated with a specific source file.

        Parent-Child æ¨¡å¼ä¸‹åŒæ­¥æ¸…ç†ï¼š
        1. ä»å‘é‡åº“åˆ é™¤å­å—ï¼ˆChild Chunksï¼‰
        2. ä» docstore åˆ é™¤å…³è”çš„çˆ¶å—ï¼ˆParent Chunksï¼‰

        çˆ¶å—æ¸…ç†ç­–ç•¥ï¼š
        - ä»å­å— metadata ä¸­æå– doc_idï¼ˆçˆ¶å— IDï¼‰
        - ä½¿ç”¨ docstore.mdelete æ‰¹é‡åˆ é™¤çˆ¶å—æ–‡ä»¶
        """
        try:
            # 1. Find IDs to delete
            # Note: LangChain's Chroma wrapper uses 'where' for metadata filtering in get()
            data = self.vectorstore.get(where={"source_file": source_file})
            ids_to_delete = data['ids']
            metadatas = data.get('metadatas', [])

            if not ids_to_delete:
                logger.warning(f"No documents found for source_file: {source_file}")
                return False

            # 2. æå–å…³è”çš„çˆ¶å— IDï¼ˆParent-Child æ¨¡å¼ï¼‰
            parent_ids_to_delete = set()
            if metadatas:
                for meta in metadatas:
                    if meta and 'doc_id' in meta:
                        parent_ids_to_delete.add(meta['doc_id'])

            # 3. Delete child chunks from vectorstore
            self.vectorstore.delete(ids=ids_to_delete)
            logger.info(f"Deleted {len(ids_to_delete)} child chunks for source_file: {source_file}")

            # 4. Delete parent chunks from docstore (Parent-Child æ¨¡å¼)
            if parent_ids_to_delete and self.parent_retriever is not None:
                try:
                    docstore = self.parent_retriever.docstore
                    # ä½¿ç”¨ mdelete æ‰¹é‡åˆ é™¤çˆ¶å—
                    docstore.mdelete(list(parent_ids_to_delete))
                    logger.info(f"Deleted {len(parent_ids_to_delete)} parent chunks from docstore")
                except Exception as e:
                    logger.warning(f"Failed to delete parent chunks from docstore: {e}")
                    # çˆ¶å—åˆ é™¤å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œå­å—å·²åˆ é™¤

            # 5. åŒæ­¥é‡å»º BM25 ç´¢å¼•
            self._build_bm25()

            return True
        except Exception as e:
            logger.error(f"Error deleting document {source_file}: {e}")
            return False
